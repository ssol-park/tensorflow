## Dockerfile
docker build -t tensorflow .

## 컨테이너 실행
docker run -it --rm --name tensorflow -v $(pwd):/app -w /app/scripts tensorflow python <파일명>
docker run -it --rm --name tensorflow -v $(pwd):/app -v $(pwd)/data:/app/data -w /app/scripts tensorflow python <파일명>


---
# 머신러닝 (ML - Machine Learning)
머신러닝은 **명시적으로 프로그래밍하지 않아도** 데이터를 통해 학습하고, 새로운 데이터에 대한 예측을 수행하는 인공지능 기술이다.
알고리즘이 데이터를 분석하여 패턴을 발견하고 이를 바탕으로 결정을 내릴 수 있다.


## 학습 방법
### 1. Supervised
- 입력 데이터와 정답(라벨)을 함께 제공하여 모델을 학습시키는 방법
- 모델이 입력 데이터에 대해 올바른 출력을 예측하도록 학습
- 스팸 메일 분류, 공부 시간 별 시험 점수 결과

### 2. Unsupervised
- 정답(라벨) 없이 입력 데이터만 제공하여 데이터를 학습하는 방법
- 데이터 내의 패턴이나 군집(클러스터)을 찾는다.
- 고객 세그먼테이션, 문서 주제 분류


## Supervised 학습 방법의 종류

### 1. Regression
- 연속적인 숫자 값을 예측하는 문제
- **수식:** h(x) = W * x + b


### 2. Binary Classification
- 두 가지 클래스 중 하나로 데이터를 분류하는 문제 (pass / non-pass)
- ex> 스팸 메일 여부 (스팸/스팸 아님), 질병 진단 (양성/음성)

### 3 Multilevel Classification
- 여러 가지 클래스 중 하나로 데이터를 분류하는 문제
- ex> 동물 분류 (고양이, 개, 새 등), 이미지 속 사물 인식

---
# 선형 회귀 모델 (Linear Regression)

## 1. 정의
독립 변수(x)와 종속 변수(y) 간의 선형 관계를 모델링하여 값을 예측하는 머신러닝 알고리즘이다.  
입력 데이터에 대해 가장 적합한 선(직선)을 찾아 예측값을 계산한다.

- 선형 회귀는 데이터의 선형적인 경향성을 기반으로 모델링하며, 독립 변수와 종속 변수 간의 관계가 선형일 때 효과적이다.
- 예: 공부 시간(x)에 따라 시험 점수(y)를 예측하거나, 집 크기(x)에 따라 가격(y)을 예측하는 문제에 활용된다.

---

## 2. 가설 (Hypothesis)
선형 회귀의 가설은 독립 변수(x)와 종속 변수(y)가 다음과 같은 선형 관계를 따른다고 가정한다:

`h(x) = W * x + b`

### 구성 요소
- **h(x)**: 예측값 (Hypothesis). 모델이 입력 데이터 x에 대해 예측한 결과를 의미한다.
- **W (Weight)**: 기울기. 독립 변수의 변화에 따른 종속 변수의 변화량을 나타낸다.
- **b (Bias)**: 절편. 독립 변수가 0일 때 종속 변수의 예상값을 나타낸다.

### 주요 특징
- 선형 관계를 직선으로 표현하며, 기울기(W)와 절편(b)이 선의 형태를 결정한다.
- 가설은 학습 데이터를 기반으로 최적의 W와 b를 찾는 과정에서 업데이트된다.

---

## 3. 비용 함수 (Cost Function)
비용 함수는 모델이 얼마나 잘 작동하는지를 평가하는 함수이다.  
선형 회귀에서는 **평균제곱오차(Mean Squared Error, MSE)** 를 비용 함수로 사용한다:

`J(W, b) = (1 / m) * Σ (h(x(i)) - y(i))^2`

### 구성 요소
- **J(W, b)**: 비용 함수 값. 예측값(h(x))과 실제값(y)의 차이를 측정한다.
- **m**: 데이터 샘플의 개수를 나타낸다.
- **h(x(i))**: i번째 데이터의 예측값이다.
- **y(i)**: i번째 데이터의 실제값이다.

### 주요 특징
1. **목적**
    - 비용 함수의 값이 작을수록 모델이 데이터를 더 잘 설명한다.
    - 학습 과정에서 비용 함수의 값을 최소화하는 것을 목표로 한다.

2. **오차 측정**
    - MSE는 예측값과 실제값 간의 차이(오차)를 제곱하여 평균을 구한다.
    - 제곱을 사용하는 이유는 오차가 양수 또는 음수일 경우 이를 모두 양수로 변환하여 처리하기 위함이다.

3. **단점**
    - 이상치(Outliers)에 민감할 수 있다. 데이터에 이상치가 포함된 경우 비용 함수 값이 왜곡될 가능성이 있다.

---

## 4. 경사 하강법 (Gradient Descent)

### 정의
경사 하강법은 비용 함수(Cost Function)를 최소화하기 위해 사용하는 최적화 알고리즘이다.  
모델의 매개변수(W와 b)를 반복적으로 업데이트하여 최적의 값을 찾는다.

---

### 1) 매개변수 업데이트 규칙
각 매개변수는 다음과 같은 규칙에 따라 업데이트된다:
- **W 업데이트**:  
  `W := W - α * (∂J(W, b) / ∂W)`

- **b 업데이트**:  
  `b := b - α * (∂J(W, b) / ∂b)`

---

### 2) 학습률 (α, Learning Rate)
- **α**는 매개변수 업데이트 크기를 조절하는 하이퍼파라미터이다.
- α 값이 너무 크면 발산할 위험이 있고, 너무 작으면 수렴 속도가 느려진다.

---

### 3) 경사 계산 (비용 함수의 편미분)
비용 함수 J(W, b)에 대한 각 매개변수의 기울기(편미분)는 다음과 같다:

- **W에 대한 편미분**:  
  `(1 / m) * Σ [(h(x(i)) - y(i)) * x(i)]`

- **b에 대한 편미분**:  
  `(1 / m) * Σ [h(x(i)) - y(i)]`

---

## 5. 전체 학습 알고리즘 흐름

1. **초기화**:
    - 매개변수 W와 b를 초기화한다. (0 또는 랜덤 값)
    - 학습률 α를 설정한다.

2. **반복 학습**:
    - 각 반복(iteration)에서:
        1. 가설 h(x)를 계산한다.
        2. 비용 함수 J(W, b)를 계산한다.
        3. 비용 함수의 기울기(경사)를 계산한다:
            - ∂J / ∂W
            - ∂J / ∂b
        4. 매개변수를 업데이트한다:
            - `W := W - α * (∂J / ∂W)`
            - `b := b - α * (∂J / ∂b)`

3. **종료 조건**:
    - 반복 횟수 도달 또는 비용 함수가 충분히 감소하면 학습을 종료한다.

---

## 6. 결과
학습을 통해 얻은 W와 b는 입력 데이터와 가장 적합한 선형 관계를 나타내며, 새로운 데이터에 대한 예측값을 계산하는 데 사용된다.

---

# 로지스틱 회귀 모델 (Logistic Regression)

## 1. 정의
로지스틱 회귀는 이진 분류 문제를 해결하기 위한 머신러닝 알고리즘이다.  
독립 변수(x)와 종속 변수(y) 간의 선형 결합을 기반으로 하며, 시그모이드(Sigmoid) 함수를 사용해 예측값을 확률로 변환한다.

- 예: 이메일이 스팸인지 아닌지, 환자가 질병에 걸렸는지 여부를 예측하는 문제에 적합하다.
- 로지스틱 회귀는 이름에 "회귀"가 포함되어 있지만, 주로 **분류 문제**에 사용된다.

---

## 2. 가설 (Hypothesis)

### 수식
로지스틱 회귀의 가설은 다음과 같다:
`h(x) = σ(W * X + b)`

- **σ(z)**: 시그모이드(Sigmoid) 함수. 선형 결합 \(W * X + b\)를 확률 값으로 변환한다.
  `σ(z) = 1 / (1 + exp(-z))`

### 구성 요소
- **h(x)**: 예측값 (Hypothesis). 입력 데이터 X에 대한 예측 확률이다.
- **W (Weight)**: 가중치 벡터. 각 독립 변수의 중요도를 나타낸다.
- **X (Input Data)**: 입력 데이터 벡터. 여러 독립 변수로 구성된다.
- **b (Bias)**: 절편. 독립 변수가 0일 때 예측값에 영향을 미치는 상수이다.

### 주요 특징
- 시그모이드 함수는 예측값을 0과 1 사이의 값으로 변환한다.
- 예측값이 0.5 이상이면 **1 (Positive Class)**, 0.5 미만이면 **0 (Negative Class)** 로 분류한다.

---

## 3. 비용 함수 (Cost Function)

로지스틱 회귀에서는 예측값이 확률이므로, **로그 손실(Log Loss)** 비용 함수를 사용한다.

### 수식
`J(W, b) = -(1 / m) * Σ [y(i) * log(h(x(i))) + (1 - y(i)) * log(1 - h(x(i)))]`

- **J(W, b)**: 비용 함수 값. 예측값(h(x))과 실제값(y)의 차이를 평가한다.
- **m**: 데이터 샘플의 개수.
- **h(x(i))**: i번째 데이터 샘플의 예측 확률.
- **y(i)**: i번째 데이터 샘플의 실제값 (0 또는 1).

### 주요 특징
1. 예측값(h(x))이 실제값(y)과 가까울수록 비용 함수 값이 작아진다.
2. 잘못된 예측일수록 비용 함수 값이 커진다.
3. 로그 손실은 확률 기반 예측에서 사용되며, 모델의 분류 성능을 최적화하는 데 적합하다.

---

## 4. 경사 하강법 (Gradient Descent)

### 정의
로지스틱 회귀에서도 비용 함수를 최소화하기 위해 **경사 하강법**을 사용한다.  
모델의 매개변수(W와 b)를 반복적으로 업데이트하여 최적의 값을 찾는다.

---

### 1) 비용 함수의 경사 계산
로지스틱 회귀의 비용 함수는 로그 손실(Log Loss)로 정의되며, 각 매개변수에 대해 기울기를 계산한다.

1. **W에 대한 편미분**:
   `∂J(W, b) / ∂W = (1 / m) * Σ [(h(x(i)) - y(i)) * X(i)]`

2. **b에 대한 편미분**:
   `∂J(W, b) / ∂b = (1 / m) * Σ [h(x(i)) - y(i)]`

---

### 2) 매개변수 업데이트
경사 하강법을 통해 매개변수를 다음과 같이 업데이트한다:

- **W 업데이트**:  
  `W := W - α * (∂J(W, b) / ∂W)`

- **b 업데이트**:  
  `b := b - α * (∂J(W, b) / ∂b)`

여기서 **α (Learning Rate)**는 매개변수 업데이트 크기를 조절한다.

---

### 주요 특징
- 경사 하강법은 비용 함수 \(J(W, b)\)를 최소화하여 최적의 \(W\)와 \(b\)를 찾는다.
- 반복(iteration)을 통해 점진적으로 매개변수를 업데이트한다.

---

## 5. 전체 학습 알고리즘 흐름

1. **초기화**:
    - 가중치(W)와 절편(b)를 초기화한다.
    - 학습률(α)을 설정한다.

2. **반복 학습**:
    - 각 반복(iteration)에서:
        1. 선형 결합 z 계산:  
           `z = W * X + b`
        2. 시그모이드 함수 적용:  
           `h(x) = σ(z)`
        3. 비용 함수 J(W, b) 계산.
        4. 비용 함수의 기울기(경사) 계산:
            - ∂J / ∂W
            - ∂J / ∂b
        5. 매개변수 업데이트:
            - `W := W - α * (∂J / ∂W)`
            - `b := b - α * (∂J / ∂b)`

3. **종료 조건**:
    - 반복 횟수 도달 또는 비용 함수 값이 충분히 감소하면 학습을 종료한다.

---

## 6. 로지스틱 회귀의 주요 특징

1. **분류 문제에 적합**:
    - 이진 분류(0/1) 문제를 해결하며, 다중 클래스 분류에도 확장 가능(소프트맥스 회귀 사용).

2. **확률 기반 예측**:
    - 시그모이드 함수로 예측값을 확률로 변환한다.
    - 확률이 0.5 이상이면 Positive Class, 그렇지 않으면 Negative Class로 분류한다.

3. **비용 함수 최적화**:
    - 로그 손실은 잘못된 예측에 대한 비용이 크기 때문에 학습 성능을 효과적으로 개선한다.

4. **경사 하강법**:
    - 비용 함수의 기울기를 계산하여 매개변수를 점진적으로 최적화한다.

---

## 7. 수식 비교: 선형 회귀 vs 로지스틱 회귀

| 구분                  | 선형 회귀                                | 로지스틱 회귀                              |
|-----------------------|-----------------------------------------|-------------------------------------------|
| 가설 (Hypothesis)     | `h(X) = W * X + b`                     | `h(X) = σ(W * X + b)`                     |
| 비용 함수 (Cost)      | 평균제곱오차(MSE)                       | 로그 손실(Log Loss)                        |
| 출력값                | 실수 값 (예: 0~무한대)                  | 확률 값 (0~1)                              |
| 목적                  | 연속형 데이터 예측                      | 이진 또는 다중 클래스 분류                  |

---

## 8. 결론
로지스틱 회귀는 선형 회귀를 확장하여 확률 기반의 분류 문제를 해결한다.  
시그모이드 함수와 로그 손실을 통해 예측값을 확률로 변환하고, 이를 기반으로 최적의 분류 모델을 학습한다.



---
## Reference
모두를 위한 딥러닝 강좌 시즌 1 - https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=1

머신 러닝의 세 가지 종류 - https://tensorflow.blog/ml-textbook/1-2-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EC%9D%98-%EC%84%B8-%EA%B0%80%EC%A7%80-%EC%A2%85%EB%A5%98/
